{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ivc.ischool.utexas.edu/VizWiz/data/Images/\n",
      "https://ivc.ischool.utexas.edu/VizWiz/data/Annotations/train.json\n",
      "https://ivc.ischool.utexas.edu/VizWiz/data/Annotations/test.json\n",
      "https://ivc.ischool.utexas.edu/VizWiz/data/Annotations/val.json\n"
     ]
    }
   ],
   "source": [
    "# (a) load VizWiz dataset\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "base_url = 'https://ivc.ischool.utexas.edu/VizWiz/data'\n",
    "img_dir = '%s/Images/' %base_url\n",
    "print(img_dir)\n",
    "\n",
    "train_split = 'train'\n",
    "train_file = '%s/Annotations/%s.json' %(base_url, train_split)\n",
    "train_data = requests.get(train_file, allow_redirects=True)\n",
    "print(train_file)\n",
    "\n",
    "test_split = 'test'\n",
    "test_file = '%s/Annotations/%s.json' %(base_url, test_split)\n",
    "test_data = requests.get(test_file, allow_redirects=True)\n",
    "print(test_file)\n",
    "\n",
    "val_split = 'val'\n",
    "val_file = '%s/Annotations/%s.json' %(base_url, val_split)\n",
    "val_data = requests.get(val_file, allow_redirects=True)\n",
    "print(val_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data: 20000\n",
      "Length of test data: 8000\n",
      "Length of validation data: 3173\n"
     ]
    }
   ],
   "source": [
    "# Read the local file\n",
    "training_data = train_data.json()\n",
    "testing_data = test_data.json()\n",
    "validation_data = val_data.json()\n",
    "print(\"Length of training data:\", len(training_data))\n",
    "print(\"Length of test data:\", len(testing_data))\n",
    "print(\"Length of validation data:\", len(validation_data))\n",
    "\n",
    "image_name_train = []\n",
    "question_train = []\n",
    "label_train = []\n",
    "\n",
    "image_name_val = []\n",
    "question_val = []\n",
    "label_val = []\n",
    "\n",
    "image_name_test = []\n",
    "question_test = []\n",
    "label_test = []\n",
    "\n",
    "num_train_VQs = 20000\n",
    "for vq in training_data[0:num_train_VQs]:\n",
    "    image_name_train.append(vq['image'])\n",
    "    question_train.append(vq['question'])\n",
    "    label_train.append(vq['answerable'])\n",
    "    \n",
    "num_val_VQs = 8000\n",
    "for vq in validation_data[0:num_val_VQs]:\n",
    "    image_name_val.append(vq['image'])\n",
    "    question_val.append(vq['question'])\n",
    "    label_val.append(vq['answerable'])\n",
    "    \n",
    "num_test_VQs = 3173\n",
    "for vq in testing_data[0:num_test_VQs]:\n",
    "    image_name_test.append(vq['image'])\n",
    "    question_test.append(vq['question'])\n",
    "#     label_test.append(vq['answerable'])\n",
    "\n",
    "import pandas as pd\n",
    "image_name_train = pd.DataFrame(image_name_train, columns=['image'])\n",
    "image_name_val = pd.DataFrame(image_name_val, columns=['image'])\n",
    "image_name_test = pd.DataFrame(image_name_test, columns=['image'])\n",
    "question_train = pd.DataFrame(question_train, columns=['question'])\n",
    "question_val = pd.DataFrame(question_val, columns=['question'])\n",
    "question_test = pd.DataFrame(question_test, columns=['question'])\n",
    "\n",
    "X_train = pd.concat([image_name_train, question_train], axis=1)\n",
    "y_train = pd.DataFrame(label_train, columns=['label'])\n",
    "X_val = pd.concat([image_name_val, question_val], axis=1)\n",
    "y_val = pd.DataFrame(label_val, columns=['label'])\n",
    "X_test = pd.concat([image_name_test, question_test], axis=1)\n",
    "# y_test = pd.DataFrame(label_test, columns='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get number 12500\n",
      "get number 13000\n",
      "get number 13500\n",
      "get number 14000\n",
      "get image https://ivc.ischool.utexas.edu/VizWiz/data/Images/VizWiz_train_000000014307.jpg failed\n",
      "get number 14500\n",
      "get number 15000\n",
      "get number 15500\n",
      "get image https://ivc.ischool.utexas.edu/VizWiz/data/Images/VizWiz_train_000000015541.jpg failed\n",
      "get number 16000\n",
      "get number 16500\n",
      "get number 17000\n",
      "get image https://ivc.ischool.utexas.edu/VizWiz/data/Images/VizWiz_train_000000017089.jpg failed\n",
      "get image https://ivc.ischool.utexas.edu/VizWiz/data/Images/VizWiz_train_000000017311.jpg failed\n",
      "get number 17500\n",
      "get image https://ivc.ischool.utexas.edu/VizWiz/data/Images/VizWiz_train_000000017821.jpg failed\n",
      "get number 18000\n",
      "get number 18500\n",
      "get image https://ivc.ischool.utexas.edu/VizWiz/data/Images/VizWiz_train_000000018603.jpg failed\n",
      "get image https://ivc.ischool.utexas.edu/VizWiz/data/Images/VizWiz_train_000000018777.jpg failed\n",
      "get image https://ivc.ischool.utexas.edu/VizWiz/data/Images/VizWiz_train_000000018938.jpg failed\n",
      "get number 19000\n",
      "get number 19500\n",
      "get image https://ivc.ischool.utexas.edu/VizWiz/data/Images/VizWiz_train_000000019757.jpg failed\n"
     ]
    }
   ],
   "source": [
    "# (b)\n",
    "# Use Microsoft Azure API to extract image-based features\n",
    "subscription_key_vision = '412bc41b5b5844febf4d7cd63510fb4f'\n",
    "vision_base_url = 'https://westcentralus.api.cognitive.microsoft.com/vision/v1.0'\n",
    "vision_analyze_url = vision_base_url + '/analyze?'\n",
    "from time import sleep\n",
    "\n",
    "def analyze_image(image_url):\n",
    "    # Microsoft API headers, params, etc\n",
    "    headers = {'Ocp-Apim-Subscription-key': subscription_key_vision}\n",
    "    params = {'visualfeatures': 'Description, Tags'}\n",
    "    data = {'url': image_url}\n",
    "    # send request, get API response\n",
    "    try:\n",
    "        response = requests.post(vision_analyze_url,headers = headers,params=params,json=data)\n",
    "    except:\n",
    "        sleep(10)\n",
    "        response = requests.post(vision_analyze_url,headers = headers,params=params,json=data)\n",
    "#     response = requests.post(vision_analyze_url, headers=headers, params=params, json=data)\n",
    "    if (response.status_code == 200):\n",
    "        analysis = response.json()\n",
    "    else:\n",
    "        print(\"get image {} failed\".format(image_url))\n",
    "        analysis = {\"description\":{\"tags\":[]}}\n",
    "    return analysis\n",
    "\n",
    "def extract_features(data):\n",
    "    return {\n",
    "        'tags': data['description']['tags'],\n",
    "#         'confidence': data['tags'][0]['confidence']\n",
    "    }\n",
    "\n",
    "image_feature = {}\n",
    "def get_image_feature(X):\n",
    "\n",
    "    for i in range(12367, 20000):\n",
    "        image_url = img_dir + '%s' %(X['image'][i])\n",
    "        data = extract_features(analyze_image(image_url))\n",
    "        tag_i = []\n",
    "        for item in data['tags']:\n",
    "            tag_i.append(item)\n",
    "        tag_i_join = ' '.join(tag_i)\n",
    "#         image_feature.append(tag_i_join)\n",
    "        image_feature[str(i)] = tag_i_join\n",
    "        if (i%500==0):\n",
    "            print('get number',str(i))\n",
    "            \n",
    "    return image_feature\n",
    "image_feature = get_image_feature(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  image feature to csv file\n",
    "\n",
    "import csv\n",
    "\n",
    "data = pd.DataFrame()\n",
    "indexlist = []\n",
    "featurelist = []\n",
    "for index,feature in image_feature.items():\n",
    "    indexlist.append(index)\n",
    "    featurelist.append(feature)\n",
    "data[\"id\"] = indexlist\n",
    "data[\"image_feature\"] = featurelist\n",
    "data.columns = [\"id\", \"image_feature\"]\n",
    "data.head()\n",
    "data.to_csv('image_feature_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text features using Microsoft Azure\n",
    "from time import sleep\n",
    "subscription_key_text = 'e25225c679e74f61a2ab61924b41a866'\n",
    "text_analytics_base_url = 'https://centralus.api.cognitive.microsoft.com/text/analytics/v2.0/'\n",
    "key_phrase_api_url = text_analytics_base_url + 'keyPhrases'\n",
    "question_feature = {}\n",
    "def get_question_feature(question_train):\n",
    "    \n",
    "    for i in range(20000):\n",
    "        \n",
    "        question_json = question_train['question'][i]\n",
    "        documents = {'documents': [{'id': i, 'text': question_json}]}\n",
    "        headers = {\"Ocp-Apim-Subscription-Key\": subscription_key_text}\n",
    "        maxiter = 10\n",
    "\n",
    "        try:\n",
    "            response = requests.post(key_phrase_api_url,headers = headers,json=documents)\n",
    "        except:\n",
    "            sleep(10)\n",
    "            response = requests.post(key_phrase_api_url,headers = headers,json=documents)\n",
    "        if(response.status_code == 200):\n",
    "            question_json = response.json()['documents']\n",
    "            question = pd.DataFrame(question_json)['keyPhrases']\n",
    "            question = question.tolist()[0]\n",
    "            tag_i=[]\n",
    "            for item in question:\n",
    "                tag_i.append(item)\n",
    "            question = ' '.join(tag_i)\n",
    "            question_feature[str(i)] = question\n",
    "        else:\n",
    "            print(\"not get\",str(i))\n",
    "            question_feature[str(i)] = \"\"\n",
    "        if (i%500==0):\n",
    "            print('get number',str(i))\n",
    "            \n",
    "    return question_feature\n",
    "question_feature = get_question_feature(X_train)\n",
    "#print(question_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write key phrase to csv file\n",
    "data = pd.DataFrame()\n",
    "indexlist = []\n",
    "keywordlist = []\n",
    "for index,keyword in question_feature.items():\n",
    "    indexlist.append(index)\n",
    "    keywordlist.append(keyword)\n",
    "data[\"id\"] = indexlist\n",
    "data[\"question_keyword\"] = keywordlist\n",
    "data.columns = [\"id\", \"question_keyword\"]\n",
    "data.head()\n",
    "data.to_csv('question_feature_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question_train['question'][5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(question_train['question']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question_feature)\n",
    "wei shenme zheme duo kong guanjianci\n",
    "hao xiang jiu zhe yang zi\n",
    "wokan nageren de ye shi\n",
    "najiuzheyangba\n",
    "hhh hounishihi ba\n",
    "yinggai keyi le\n",
    "xianzai pao?dui \n",
    "buguo xiamain de xiewenjian yao gai\n",
    "wo zai ba ta zhuancheng yige liebiao?\n",
    "weisha yao zhuan lai zhuan qu de?\n",
    "yinwei fu he ni xianyou de daima \n",
    "ruguo shiwo  wo  csv buhui zhe yang xie \n",
    "xiang kan ni hui zenme xie csv\n",
    "wo xiege json suanle \n",
    "jiandan cubao\n",
    "na wo yuan lai yong de pandas\n",
    "hui he json chong tu ma\n",
    "shayisi\n",
    "houmian de ?\n",
    "pd.dataframe and json\n",
    "hui chongtu ma\n",
    "bu zhidao ni zhege chongtu shayisi\n",
    "pd keyi du jsonba\n",
    "ok\n",
    "lai ba\n",
    "bu dui wo shi yixia chongxie zhege csv\n",
    "ok?\n",
    "di yi lie shi xiabiao\n",
    "dierlie shi guanjainci\n",
    "keyi!\n",
    "na wo shi bushi ba image nage ye zhe yang gai?\n",
    "keyi\n",
    "ranhou keyi he bing dao yige biaoli\n",
    "ruguo nigaoxingdehua\n",
    "\n",
    "haiyouge wen ti\n",
    "shi bu shi yao ba train test val doupaochulai?shide\n",
    "ma ya!!!!!!\n",
    "xiuxiuxiu\n",
    "ni xie ge sancheng for fangna ziji pao bujiuxingle ?\n",
    "oooooooooo\n",
    "shoudong ye xing\n",
    "ni zi ji xian nong ba \n",
    "ni cun de shihou biao ming bie wangle gai\n",
    "buran fugaile\n",
    "liao jieok\n",
    "na wo chele\n",
    "hou\n",
    "\n",
    "ganggang wo kan ni shan diao le yi ju hua\n",
    "na ge keyi buyao ma\n",
    "nage hua jiushi ruguo  huo qu guocheng zhong fasheng cuowu baoduode\n",
    "\n",
    "wo gaiwei print shuchu le\n",
    "zhe yang daima bu hui ting\n",
    "nipaodao zuihou  kankan you duoshao cuo\n",
    "soga\n",
    "na wo xian zai pao 773-20000?keyi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text features using Microsoft Azure\n",
    "subscription_key_text = '5ab635de9b754ffdb72dfbd4cf1726df'\n",
    "text_analytics_base_url = 'https://centralus.api.cognitive.microsoft.com/text/analytics/v2.0/'\n",
    "key_phrase_api_url = text_analytics_base_url + 'keyPhrases'\n",
    "\n",
    "def get_question_feature(question_train):\n",
    "    question_feature = []\n",
    "    for i in range(2000):\n",
    "        question_json = question_train['question'][i]\n",
    "        documents = {'documents': [{'id': i, 'text': question_json}]}\n",
    "        headers = {\"Ocp-Apim-Subscription-Key\": subscription_key_text}\n",
    "        response = requests.post(key_phrase_api_url,headers = headers,json=documents)\n",
    "        question_json = response.json()['documents']\n",
    "        question = pd.DataFrame(question_json)['keyPhrases']\n",
    "        question = question.tolist()[0]\n",
    "        tag_i=[]\n",
    "        for item in question:\n",
    "            tag_i.append(item)\n",
    "        question = ' '.join(tag_i)\n",
    "        question_feature.append(question)\n",
    "    \n",
    "    return question_feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
